<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="7v_qh-WhWfxbdJGWwg1I2FccnF-ad61wbH_MnWGvyjM"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>research | Piersilvio De Bartolomeis</title> <meta name="author" content="Piersilvio De Bartolomeis"/> <meta name="description" content="papers by categories in reversed chronological order. [*] denotes equal contribution."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/tomato.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://pdebartol.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">PiersilvioÂ </span>De Bartolomeis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">research<span class="sr-only">(current)</span></a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">research</h1> <p class="post-description">papers by categories in reversed chronological order. [*] denotes equal contribution.</p> </header> <article> <h4 id="preprints">preprints</h4> <div class="publications"> <ol class="bibliography"> <li> <div class="row col-sm"> <div id="karlsson2025}" class="col-sm"> <div class="title">Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data</div> <div class="author"> Rickard Karlsson, Piersilvio De Bartolomeis, Issa J. Dahabreh, and Jesse H. Krijthe </div> <div class="periodical"> <em>arXiv preprint,</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2507.03681" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Randomized trials are typically designed to detect average treatment effects but often lack the statistical power to uncover effect heterogeneity over patient characteristics, limiting their value for personalized decision-making. To address this, we propose the QR-learner, a model-agnostic learner that estimates conditional average treatment effects (CATE) within the trial population by leveraging external data from other trials or observational studies. The proposed method is robust: it has the potential to reduce the CATE prediction mean squared error while maintaining consistency, even when the external data is not aligned with the trial. Moreover, we introduce a procedure that combines the QR-learner with a trial-only CATE learner and show that it asymptotically matches or exceeds the trial-only learner in terms of mean squared error. We examine the performance of our approach in simulation studies and apply the methods to a real-world dataset, demonstrating improvements in both CATE estimation and statistical power for detecting heterogeneous effects. </p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="demirel2025}" class="col-sm"> <div class="title">Uncovering Bias Mechanisms in Observational Studies</div> <div class="author"> Ilker Demirel, Zeshan Hussain, Piersilvio De Bartolomeis, and David Sontag </div> <div class="periodical"> <em>arXiv preprint,</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2506.01191" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Observational studies are a key resource for causal inference but are often affected by systematic biases. Prior work has focused mainly on detecting these biases, via sensitivity analyses and comparisons with randomized controlled trials, or mitigating them through debiasing techniques. However, there remains a lack of methodology for uncovering the underlying mechanisms driving these biases, e.g., whether due to hidden confounding or selection of participants. In this work, we show that the relationship between bias magnitude and the predictive performance of nuisance function estimators (in the observational study) can help distinguish among common sources of causal bias. We validate our methodology through extensive synthetic experiments and a real-world case study, demonstrating its effectiveness in revealing the mechanisms behind observed biases. Our framework offers a new lens for understanding and characterizing bias in observational studies, with practical implications for improving causal inference.</p> </div> </div> </div> </li> </ol> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <ol class="bibliography"></ol> </div> <h4 id="publications">publications</h4> <div class="publications"> <ol class="bibliography"> <li> <div class="row col-sm"> <div id="dahabreh2025trial" class="col-sm"> <div class="title">Trial Emulation, Simulation, and Augmentation Using Electronic Health Records and Generative AI</div> <div class="author"> Issa J. Dahabreh, Robert W. Yeh, and Piersilvio De Bartolomeis </div> <div class="periodical"> <em>NEJM AI,</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://ai.nejm.org/doi/full/10.1056/AIe2500894" class="pdf btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> <div class="abstract hidden"> <p>Computational tools, such as TRIALSCOPE, can support target trial emulations by leveraging biomedical language models to automate information extraction from unstructured data, conduct data validation, and streamline statistical analysis and model assessment. These advances promise to improve the quality of research using electronic health record data for causal inference. Nevertheless, automation cannot, on its own, address biases inherent in the process of generating observational data, such as unmeasured confounding. When these biases affect emulation results and a new randomized trial is deemed necessary, emulations aided by automation may still be valuable for informing trial design via realistic simulations, and for improving the efficiency of randomized trials via trial augmentation methods that combine the power of randomization with large-scale observational data.</p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="debartolomeis2025efficient" class="col-sm"> <div class="title">Efficient Randomized Experiments Using Foundation Models</div> <div class="author"> Piersilvio De Bartolomeis, Javier Abad, Guanbo Wang, Konstantin Donhauser, Raymond M. Duch, Fanny Yang, and Issa J. Dahabreh </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS),</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2502.04262" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/efficient_rcts_posters.pdf" class="poster btn-sm z-depth-0" role="button">poster</a> <a href="https://carswg.github.io/journalclub/journalclub12.html" class="video btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">talk</a> <a href="https://github.com/jaabmar/HAIPW" class="code btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">code</a> </div> <div class="abstract hidden"> <p>Randomized experiments are the preferred approach for evaluating the effects of interventions, but they are costly and often yield estimates with substantial uncertainty. On the other hand, in silico experiments leveraging foundation models offer a cost-effective alternative that can potentially attain higher statistical precision. However, the benefits of in silico experiments come with a significant risk: statistical inferences are not valid if the models fail to accurately predict experimental responses to interventions. In this paper, we propose a novel approach that integrates the predictions from multiple foundation models with experimental data while preserving valid statistical inference. Our estimator is consistent and asymptotically normal, with asymptotic variance no larger than the standard estimator based on experimental data alone. Importantly, these statistical properties hold even when model predictions are arbitrarily biased. Empirical results across several randomized experiments show that our estimator offers substantial precision gains, equivalent to a reduction of up to 20% in the sample size needed to match the same precision as the standard estimator based on experimental data alone.</p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="cadei2025causal" class="col-sm"> <div class="title">Prediction-Powered Causal Inferences</div> <div class="author"> Riccardo Cadei, Ilker Demirel*, Piersilvio De Bartolomeis*, Lukas Lindorfer, Sylvia Cremer, Cordelia Schmid, and Francesco Locatello </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS),</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2502.06343" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations. We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled similar experiment. First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples. Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, âCausal Liftingâ the predictive model. We propose Deconfounded Empirical Risk Minimization (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects. We validate our method on both synthetic and real-world scientific data. Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant.</p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="de2025doubly" class="col-sm"> <div class="title">Doubly robust identification of treatment effects from multiple environments</div> <div class="author"> Piersilvio De Bartolomeis, Julia Kostin, Javier Abad, Yixin Wang, and Fanny Yang </div> <div class="periodical"> <em>International Conference on Learning Representations (ICLR),</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="http://arxiv.org/abs/2503.14459" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster_iclr25.pdf" class="poster btn-sm z-depth-0" role="button">poster</a> <a href="https://github.com/jaabmar/ramen" class="code btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">code</a> </div> <div class="abstract hidden"> <p>Practical and ethical constraints often dictate the use of observational data for causal inference, particularly in medicine and social sciences. Yet, observational datasets are prone to confounding, potentially compromising the validity of conclusions. While adjusting for all available covariates is a common corrective strategy, this approach can introduce bias, especially when post-treatment variables are present or some variables remain unobservedâa frequent scenario in practice. Avoiding this bias often requires detailed knowledge of the underlying causal graph, a challenging and often impractical prerequisite. In this work, we propose RAMEN, an algorithm that tackles this challenge by leveraging the heterogeneity of multiple data sources without the need to know the complete causal graph. Notably, RAMEN achieves doubly robust identification: we identify the treatment effect if either the causal parents of the treatment or those of the outcome are observed. Empirical evaluations across synthetic, semi-synthetic, and real-world datasets show that our approach significantly outperforms existing methods.</p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="karlsson2025robustintegration" class="col-sm"> <div class="title">Robust integration of external control data in randomized trials</div> <div class="author"> Rickard Karlsson, Guanbo Wang, Piersilvio De Bartolomeis, Jesse H. Krijthe, and Issa J. Dahabreh </div> <div class="periodical"> <em>Biometrics,</em> 2025 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2406.17971" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>One approach for increasing the efficiency of randomized trials is the use of "external controls" â individuals who received the control treatment studied in the trial during routine practice or in prior experimental studies. Existing external control methods, however, can be biased if the populations underlying the trial and the external control data are not exchangeable. Here, we characterize a randomization-aware class of treatment effect estimators in the population underlying the trial that remain consistent and asymptotically normal when using external control data, even when exchangeability does not hold. We consider two members of this class of estimators: the well-known augmented inverse probability weighting trial-only estimator, which is the efficient estimator when only trial data are used; and a potentially more efficient member of the class when exchangeability holds and external control data are available, which we refer to as the optimized randomization-aware estimator. To achieve robust integration of external control data in trial analyses, we then propose a combined estimator based on the efficient trial-only estimator and the optimized randomization-aware estimator. We show that the combined estimator is consistent and no less efficient than the most efficient of the two component estimators, whether the exchangeability assumption holds or not. We examine the estimatorsâ performance in simulations and we illustrate their use with data from two trials of paliperidone extended-release for schizophrenia.</p> </div> </div> </div> </li> </ol> <ol class="bibliography"> <li> <div class="row col-sm"> <div id="de2024detecting" class="col-sm"> <div class="title">Detecting critical treatment effect bias in small subgroups</div> <div class="author"> Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, and Fanny Yang </div> <div class="periodical"> <em>Conference on Uncertainty in Artificial Intelligence (UAI),</em> 2024 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2404.18905" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster_uai24.pdf" class="poster btn-sm z-depth-0" role="button">poster</a> <a href="https://github.com/jaabmar/kernel-test-bias" class="code btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">code</a> </div> <div class="abstract hidden"> <p>Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.</p> </div> </div> </div> </li> <li> <div class="row col-sm"> <div id="debartolomeis2023hidden" class="col-sm"> <div class="title">Hidden yet quantifiable: A lower bound for confounding strength using randomized trials</div> <div class="author"> Piersilvio De Bartolomeis*, Javier Abad*, Konstantin Donhauser, and Fanny Yang </div> <div class="periodical"> <em>International Conference on Artificial Intelligence and Statistics (AISTATS),</em> 2024 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2312.03871" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/poster_conf.pdf" class="poster btn-sm z-depth-0" role="button">poster</a> <a href="https://github.com/jaabmar/confounder-lower-bound" class="code btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">code</a> </div> <div class="abstract hidden"> <p>In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new drugs in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions from observational data. We propose a novel strategy to quantify unobserved confounding by leveraging randomized trials. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.</p> </div> </div> </div> </li> </ol> <ol class="bibliography"><li> <div class="row col-sm"> <div id="mutti2023convex" class="col-sm"> <div class="title">Convex Reinforcement Learning in Finite Trials</div> <div class="author"> Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli </div> <div class="periodical"> <em>Journal of Machine Learning Research (JMLR),</em> 2023 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://www.jmlr.org/papers/volume24/22-1514/22-1514.pdf" class="pdf btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> <div class="abstract hidden"> <p>Convex Reinforcement Learning (RL) is a recently introduced framework that generalizes the standard RL objective to any convex (or concave) function of the state distribution induced by the agentâs policy. This framework subsumes several applications of practical interest, such as pure exploration, imitation learning, and risk-averse RL, among others. However, the previous convex RL literature implicitly evaluates the agentâs performance over infinite realizations (or trials), while most of the applications require excellent performance over a handful, or even just one, trials. To meet this practical demand, we formulate convex RL in finite trials, where the objective is any convex function of the empirical state distribution computed over a finite number of realizations. In this paper, we provide a comprehensive theoretical study of the setting, which includes an analysis of the importance of non-Markovian policies to achieve optimality, as well as a characterization of the computational and statistical complexity of the problem in various configurations.</p> </div> </div> </div> </li></ol> <ol class="bibliography"><li> <div class="row col-sm"> <div id="mutti2022challenging" class="col-sm"> <div class="title">Challenging Common Assumptions in Convex Reinforcement Learning</div> <div class="author"> Mirco Mutti*, Riccardo De Santi*, Piersilvio De Bartolomeis, and Marcello Restelli </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS),</em> 2022 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2202.01511" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="/assets/pdf/neurips_convex_poster.pdf" class="poster btn-sm z-depth-0" role="button">poster</a> </div> <div class="abstract hidden"> <p>The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives are equivalent and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in convex RL. In particular, we prove that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others.</p> </div> </div> </div> </li></ol> </div> <h4 id="workshops">workshops</h4> <div class="publications"> <ol class="bibliography"></ol> <ol class="bibliography"></ol> <ol class="bibliography"><li> <div class="row col-sm"> <div id="debartolomeis2023how" class="col-sm"> <div class="title">How robust accuracy suffers from certified training with convex relaxations</div> <div class="author"> Piersilvio De Bartolomeis, Jacob Clarysse, Amartya Sanyal, and Fanny Yang </div> <div class="periodical"> <em>I Canât Believe Itâs Not Better Workshop (NeurIPS), Oral, </em> 2023 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://arxiv.org/abs/2306.06995" class="arxiv btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> </div> <div class="abstract hidden"> <p>Adversarial attacks pose significant threats to deploying state-of-the-art classifiers in safety-critical applications. Two classes of methods have emerged to address this issue: empirical defences and certified defences. Although certified defences come with robustness guarantees, empirical defences such as adversarial training enjoy much higher popularity among practitioners. In this paper, we systematically compare the standard and robust error of these two robust training paradigms across multiple computer vision tasks. We show that in most tasks and for both \(\ell_â\)-ball and \(\ell_2\)-ball threat models, certified training with convex relaxations suffers from worse standard and robust error than adversarial training. We further explore how the error gap between certified and adversarial training depends on the threat model and the data distribution. In particular, besides the perturbation budget, we identify as important factors the shape of the perturbation set and the implicit margin of the data distribution. We support our arguments with extensive ablations on both synthetic and image datasets.</p> </div> </div> </div> </li></ol> <ol class="bibliography"><li> <div class="row col-sm"> <div id="rds2022" class="col-sm"> <div class="title">Enhancing Unit-tests for Invariance Discovery</div> <div class="author"> Piersilvio De Bartolomeis, Antonio Orvieto, and Giambattista Parascandolo </div> <div class="periodical"> <em>Spurious Correlations, Invariance, and Stability Workshop (ICML),</em> 2022 </div> <div class="links"> <a class="abstract btn-sm z-depth-0" role="button">abstract</a> <a href="https://openreview.net/pdf?id=-XVMGVmjNLs" class="pdf btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">pdf</a> </div> <div class="abstract hidden"> <p>Recently, Aubin et al. (2021) proposed a set of linear low-dimensional problems to precisely evaluate different types of out-of-distribution generalization. In this paper, we show that one of these problems can already be solved by established algorithms, simply by better hyper-parameter tuning. We then propose an enhanced version of the linear unit-tests. To the best of our hyperparameter search and within the set of algorithms evaluated, AND-mask is the best performing algorithm on this new suite of tests. Our findings on synthetic data are further reinforced by experiments on an image classification task where we introduce spurious correlations.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Piersilvio De Bartolomeis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-M7TH7B3K2F"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-M7TH7B3K2F");</script> </body> </html>