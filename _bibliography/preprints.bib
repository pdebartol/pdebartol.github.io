@article{karlsson2025},
  title={Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data},
  author={Rickard Karlsson and Piersilvio De Bartolomeis and Issa J. Dahabreh and Jesse H. Krijthe},
  conference={arXiv preprint,},
  year={2025},
  arxiv={https://arxiv.org/abs/2502.06343},
  abstract={Randomized trials are typically designed to detect average treatment effects but often lack the statistical power to uncover effect heterogeneity over patient characteristics, limiting their value for personalized decision-making. To address this, we propose the QR-learner, a model-agnostic learner that estimates conditional average treatment effects (CATE) within the trial population by leveraging external data from other trials or observational studies. The proposed method is robust: it has the potential to reduce the CATE prediction mean squared error while maintaining consistency, even when the external data is not aligned with the trial. Moreover, we introduce a procedure that combines the QR-learner with a trial-only CATE learner and show that it asymptotically matches or exceeds the trial-only learner in terms of mean squared error. We examine the performance of our approach in simulation studies and apply the methods to a real-world dataset, demonstrating improvements in both CATE estimation and statistical power for detecting heterogeneous effects.
}
}
@article{demirel2025},
  title={Uncovering Bias Mechanisms in Observational Studies},
  author={Ilker Demirel and Zeshan Hussain and Piersilvio De Bartolomeis and David Sontag},
  conference={arXiv preprint,},
  year={2025},
  arxiv={https://arxiv.org/abs/2506.01191},
  abstract={Observational studies are a key resource for causal inference but are often affected by systematic biases. Prior work has focused mainly on detecting these biases, via sensitivity analyses and comparisons with randomized controlled trials, or mitigating them through debiasing techniques. However, there remains a lack of methodology for uncovering the underlying mechanisms driving these biases, e.g., whether due to hidden confounding or selection of participants. In this work, we show that the relationship between bias magnitude and the predictive performance of nuisance function estimators (in the observational study) can help distinguish among common sources of causal bias. We validate our methodology through extensive synthetic experiments and a real-world case study, demonstrating its effectiveness in revealing the mechanisms behind observed biases. Our framework offers a new lens for understanding and characterizing bias in observational studies, with practical implications for improving causal inference.}

}


