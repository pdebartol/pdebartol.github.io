@article{de2024detecting,
      title={Detecting critical treatment effect bias in small subgroups}, 
      author={ Piersilvio De Bartolomeis and Javier Abad and Konstantin Donhauser and Fanny Yang},
      year={2024},
     conference={Conference on Uncertainty in Artificial Intelligence (UAI),},
    arxiv={https://arxiv.org/abs/2404.18905},
    code={https://github.com/jaabmar/kernel-test-bias},
    poster={poster_uai24.pdf},
     abstract={Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice.  Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. 
We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study.  Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.}
}

@article{de2025doubly,
      title={Doubly robust identification of treatment effects from multiple environments}, 
      author={ Piersilvio De Bartolomeis and Julia Kostin and Javier Abad and  Yixin Wang and Fanny Yang},
      year={2025},
     conference={International Conference on Learning Representations (ICLR),},
    pdf={https://openreview.net/pdf?id=9vTAkJ9Tik}}


@article{debartolomeis2023hidden,
      title={Hidden yet quantifiable: A lower bound for confounding strength using randomized trials}, 
      author={ Piersilvio De Bartolomeis* and Javier Abad* and Konstantin Donhauser and Fanny Yang},
      year={2024},
     conference = {International Conference on Artificial Intelligence and Statistics (AISTATS),},
      arxiv={https://arxiv.org/abs/2312.03871},
      code={https://github.com/jaabmar/confounder-lower-bound},
      poster={poster_conf.pdf},
     abstract={In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new drugs in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions from observational data. We propose a novel strategy to quantify unobserved confounding by leveraging randomized trials. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.}
}


@article{mutti2023convex,
      title={Convex Reinforcement Learning in Finite Trials}, 
      author={Mirco Mutti and Riccardo De Santi and Piersilvio De Bartolomeis and Marcello Restelli},
      year={2023},
     conference = {Journal of Machine Learning Research (JMLR),},
     pdf={https://www.jmlr.org/papers/volume24/22-1514/22-1514.pdf},
      abstract={Convex Reinforcement Learning (RL) is a recently introduced framework that generalizes the standard RL objective to any convex (or concave) function of the state distribution induced by the agent's policy. This framework subsumes several applications of practical interest, such as pure exploration, imitation learning, and risk-averse RL, among others. However, the previous convex RL literature implicitly evaluates the agent's performance over infinite realizations (or trials), while most of the applications require excellent performance over a handful, or even just one, trials.
To meet this practical demand, we formulate convex RL in finite trials, where the objective is any convex function of the empirical state distribution computed over a finite number of realizations. In this paper, we provide a comprehensive theoretical study of the setting, which includes an analysis of the importance of non-Markovian policies to achieve optimality, as well as a characterization of the computational and statistical complexity of the problem in various configurations.},
}



@article{mutti2022challenging,
      title={Challenging Common Assumptions in Convex Reinforcement Learning}, 
      author={Mirco Mutti* and Riccardo De Santi* and Piersilvio De Bartolomeis and Marcello Restelli},
      year={2022},
     conference = {Advances in {Neural} {Information} {Processing} {Systems} (NeurIPS),},
      abstract={The classic Reinforcement Learning (RL) formulation concerns the maximization of a scalar reward function. More recently, convex RL has been introduced to extend the RL formulation to all the objectives that are convex functions of the state distribution induced by a policy. Notably, convex RL covers several relevant applications that do not fall into the scalar formulation, including imitation learning, risk-averse RL, and pure exploration. In classic RL, it is common to optimize an infinite trials objective, which accounts for the state distribution instead of the empirical state visitation frequencies, even though the actual number of trajectories is always finite in practice. This is theoretically sound since the infinite trials and finite trials objectives are equivalent and thus lead to the same optimal policy. In this paper, we show that this hidden assumption does not hold in convex RL. In particular, we prove that erroneously optimizing the infinite trials objective in place of the actual finite trials one, as it is usually done, can lead to a significant approximation error. Since the finite trials setting is the default in both simulated and real-world RL, we believe shedding light on this issue will lead to better approaches and methodologies for convex RL, impacting relevant research areas such as imitation learning, risk-averse RL, and pure exploration among others.},
      arxiv={https://arxiv.org/abs/2202.01511},
      poster={neurips_convex_poster.pdf}
}


